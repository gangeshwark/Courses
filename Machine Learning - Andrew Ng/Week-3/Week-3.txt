- Applying linear-regression to classification problems is not a great idea.

- Linear-regression doesn't deal well with outliers.

- Logistic regression is a classification algorithm.

- Logistic function/Sigmoid function is used in Logistic regression

- h(x) = 1/(1+e^(-Theta'*X))

- Interpreting h(x) as estimated probability that y=1 on input x.

- Non linear decision boundaries, you need add higher order polynomials

- In statistics the process of maximum-likelihood estimation is the process of estimating the parameters of statistical model given data.

- Applying feature scaling can help for Logistic Regression also.

- Gradient Descent, BFGS, L-BFGS are optimization algorithms which need methods to calculate cost of function and partial derivative of cost and then
  can achieve the same objective as Gradient descent.
- For the optimization techniques there is no need to find learning rate. Use line-search to find out learning rate.
- Often converge faster than gradient descent, they do more sophisticated techniques.

- Using advanced optimization techniques in Octave:
    function [jval, gradient] = costFunction(theta);
    options = optimset('GradObj', 'on', 'MaxIter', 100);
    initialTheta = zeros(2,1);
    [optTheta, funcVal, exitFlag] = fminunc(@costFunction, initialTheta, options);
    % @ represent a pointer to the function

- Overfitting:
    If we have too many features, the learned hypothesis may fit the training set very well but may fail to generalize to new examples.

    If the model doesn't fit the training set really well. This is called underfitting. This is also called that the model has a bias.
    Bias also means that the algorithm has a preconception.

    When model is really customized to the training points. It has a high-variance. Not enough data to give us a good hypothesis.

- Addressing overfitting:
    - Reduce number of features to keep.
        Model selection algorithm.
    - Regularization
        Keep all of the features but reduce the magnitude of Thetaj
        Works well when we have a lot of features each of which contributes a bit to predicting y.

- Having small values for all parameters leads to simpler models and less vulnerability to overfitting.

- In regularization we don't penalize Theta0.

- The term 1-alpha*lambda/m is going to be slightly less than 1.

- In regularized gradient descent we are shrinking the theta parameter a little bit with each iteration of gradient descent.

- It can be proved that as long as the regularization constant is > 0 the matrix whose inverse is required for normal equations is likely
  invertible.
